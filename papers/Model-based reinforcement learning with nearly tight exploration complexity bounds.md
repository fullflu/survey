# Model-based reinforcement learning with nearly tight exploration complexity bounds

<!-----------------------------------------------------------------
# サムネイル
------------------------------------------------------------------->
<!-- <img src='../tmb/template.png' width=750px /> -->


<!-----------------------------------------------------------------
# 関連情報記述欄

論文リンク・出版年以外はoptional

-------
EXAMPLE
-------

- 論文リンク: https://arxiv.org/abs/1611.01626
- 出版年: 2017
- ジャーナル・カンファレンス: ICLR
- 著者: Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, Volodymyr Mnih
- 所属: DeepMind
- 関連リンク:
- [openreview](https://openreview.net/forum?id=B1kJ6H9ex)
- タグ:
- :q-learning:
- :policy gradient:
- :atari:
- :neural network:
------------------------------------------------------------------->
- 論文リンク: http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_SzitaS10.pdf
- 出版年: 2010
- ジャーナル・カンファレンス: ICML
<!-- - 著者: -->
<!-- - 所属: -->
<!--
- 関連リンク:
-
-->
<!--
- タグ:
-
-->


<!-----------------------------------------------------------------
# 論文内容まとめ記述欄

概要以外はoptional

-------
EXAMPLE
-------

#### 概要
エントロピー正則化付きの方策勾配法とQ学習を組み合わせた新しいアルゴリズムPGQを提案し、DQNやA3Cに対する優位性をAtariドメインで実験的に示した。

#### 目的
方策勾配法は方策オン型で経験再生を使えずサンプル効率が悪いため、Q学習（方策オフ型）と組み合わせてこれを解決したい。

#### 貢献（新規性・差分）
1. エントロピー正則化付きの方策勾配法の推定している方策πが、πに基づくアドバンテージ関数Aによって表せることを示した (Sec.3.1, 3.2, Eq.4)
2. 上記の関係を用いてPGQを提案・評価した (Sec.4., 5.)
3. Actor-critic法 (e.g., ベースライン付きの方策勾配法) の更新則と行動価値ベースの手法（e.g., SARSA, Q学習）の更新則が（特殊な場合に）等価であることを示した (Sec. 3.3)

#### 手法
PGQはまず、エントロピー正則化付きの方策勾配法で推定しているπと、この方策に基づくアドバンテージAの関係 (Eq.4) を使って、方策勾配法の推定しているπとVから、πに基づくQを計算する。このQがベルマン最適方程式に従うよう正則化をかけた方策勾配法の目的関数を最適化する。この正則加項部分の最適化をQ学習と同じく経験再生を使って行う。

#### 結果

##### 1. Atariドメインでの評価
Atariの50以上のゲームにおいて、得られた報酬に基づくスコアによる評価を行い、DQNとA3Cと比較を行った。
50以上のゲームにおける平均スコアだけでなくスコアの中央値でも人間のスコアを上回り、PGQとDQNとA3Cの3アルゴリズム中最下位になったのは1つのゲームだけだった。
----------------------------------------------------------------->
## まとめ

#### 概要
有限MDPにおけるモデルベースの強化学習手法MORMAX (MOdified RMAX) を提案し，SoTAのsample complexityを達成することを証明した．訪問数が少ないところの報酬関数を上限値Rmaxに固定して探索に使うところが肝

#### 目的
強化学習の探索におけるsample complexityを改善したい．環境が有限MDPとして与えられるときのモデルベースの手法にフォーカス

#### 貢献（新規性・差分）
##### 背景
モデルベースの手法を使うとsample complexityが改善すると期待されるが，既存手法のRMAXではO(N^2LogN)しか達成できず，O(NlogN) を達成するモデルフリーな遅延Q学習より悪かった（N: 状態数）

##### 新規性
* モデルベースの手法を提案し，既存のsample complexityを改善，O(NlogN)を達成
* モデルフリーの手法とNのオーダーは同じだが，Vmax依存の項は改善
* テクニカルなバウンドの証明方法を提案（他の手法のバウンドも同様に改善できるかも）


#### 手法
##### 前提
* 状態空間，行動空間が有限なMDP（要素数は既知）
* 報酬関数は非負で有界（上限Rmaxが既知）
* 報酬関数は既知で，確率遷移カーネルは未知（報酬も未知の場合への拡張は簡単）

##### 気持ち
* 状態行動対（x,a）が十分訪問されるまでは，その対をunknownとみなし，報酬をRmaxに固定する（不確かなところが訪問されやすくなる）
* RMAXとの違いは，モデルの更新が行われる条件が増えたことと，m（knownとみなすまでのカウント数）が小さくなったことと，データがリフレッシュ（カウントがリセット）されること

##### アルゴリズム
1. 確率遷移カーネルを確率1で自己ループするように初期化
2. すべての状態行動対のカウントを0に設定し，報酬関数R^(x,a)をRmaxに初期化
3. MDPを解く（価値反復すればQ = Vmaxになるはず）
4. すべての状態行動対(x,a)について``unknown''とみなす
5. 初期状態をセットする
6. 以下のループを繰り返す
7. 現在の方策に従って行動をとり，x,a,yをカウントする
8. (x,a)を初めてm回訪問したとき，カウントを用いてP^(x,a,・)とQ^(x,a)とR^(x,a)を更新し，(x,a)と(x,a,・)のカウントを0に戻す．(x,a)を``known''とみなす (14行目)
9. knownな(x,a)のカウントがmになったとき，直前のモデル更新でQ^(x,a)が大きく変化した場合（14行目）のみ，カウントを使ってMDPを解く．解いた結果が17行目の条件を満たせばP^(x,a,・)とQ(・,・)を更新．最後にカウントを0に戻す


<!--
#### 結果

##### 1.

##### 2.
-->

#### 定理・証明していること（汎用的で重要なものであれば）
* sample compleixtyをS.C.と書く．S.C.は「1-δ以上の確率で，期待収益が最適な収益と比べてεより小さくなる時刻の数が高々S.C.」であるような数として定義
  - εはaccuracy，δは失敗率を表す
  - それっぽい式で書くと $\Pr[\sum_{t} \mathbb{I}_{V_{t,M}^{\mathcal{A}} - V^*(x_t) - \epsilon} \le S.C.] \ge 1-\delta$
* **定理1**: S.C.はO(NlogN)で抑えられる
* **証明**: 3.3のFの定義あたりからよくわからなくなった（教えてほしい・・）
  <!-- -  それっぽい式で書くと $^{forall}n \ge S.C., \Pr[\sum_{t} \mathbb{I}_{V_{t,M}^{\mathcal{A}} - V^*(x_t) - \epsilon} \le n] \ge 1-\delta$ （要検証）-->


## 次に読むべき論文
"Sample Complexity Bounds of Exploration" http://www.research.cs.rutgers.edu/~lihong/pub/Li12Sample.pdf

<!-----------------------------------------------------------------
# コメント欄

コメントの他に
- コメント記述者: @XXX
- 点数: X/10（論文が必読に値するかどうかを10点満点で書く）
を書く。

コメントは記述者・点数が分かればあとはフォーマットは自由
返信なども可（例: そうかもしれないですね by @XXX）

-------
EXAMPLE
-------

#### @sotetsuk: 8/10
- 方策勾配法はナイーブな定式化では探索をすることができずに方策が決定論的になりがちだが、探索を促すエントロピー正則化を使った方策勾配法がある意味でより自然な定式化かもしれない、という示唆とも捉えることができて面白い。
- Eq.4からπとVだけを使って（妥当な）Qを計算しているのがPGQのポイントだと思った。
- そうかもしれないですね by @XXX
----------------------------------------------------------------->
## コメント

<!-- 1人目 -->
#### @fullflu: 8/10

- 「モデルベースの手法はO(N^2)の壁を越えられないのではないか」という当時の仮説を打ち破った研究で，理論研究としての価値は高い
- "実用的には，古いサンプルをリセットしない方がいいかもしれないし，mをもっと小さくすべきかもしれない" と論文で言っていて，まぁせやなという感じ
- 理論が好きな人には良いが，そうでない人には古くて微妙かも
- 証明がノーテーション地獄で追えない・・

<!-- 2人目 -->
<!--
#### @XXX: X/10
-
-->

<!-- 3人目 -->
<!--
#### @XXX: X/10
-
-->
